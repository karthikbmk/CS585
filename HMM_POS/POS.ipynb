{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part of Speech Tagging using HMMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import os.path\n",
    "import urllib.request\n",
    "\n",
    "class HMM:\n",
    "\tdef __init__(self, smoothing=0):\n",
    "\t\t\"\"\"\n",
    "\t\tConstruct an HMM model with add-k smoothing.\n",
    "\t\tParams:\n",
    "\t\t  smoothing...the add-k smoothing value\n",
    "\t\t\n",
    "\t\tThis is DONE.\n",
    "\t\t\"\"\"\n",
    "\t\tself.smoothing = smoothing\n",
    "\n",
    "\tdef fit_transition_probas(self, tags):\n",
    "\t\t\"\"\"\n",
    "\t\tEstimate the HMM state transition probabilities from the provided data.\n",
    "\n",
    "\t\tCreates a new instance variable called `transition_probas` that is a \n",
    "\t\tdict from a string ('state') to a dict from string to float. E.g.\n",
    "\t\t{'N': {'N': .1, 'V': .7, 'D': 2},\n",
    "\t\t 'V': {'N': .3, 'V': .5, 'D': 2},\n",
    "\t\t ...\n",
    "\t\t}\n",
    "\t\tSee test_hmm_fit_transition.\n",
    "\t\t\n",
    "\t\tParams:\n",
    "\t\t  tags...a list of lists of strings representing the tags for one sentence.\n",
    "\t\tReturns:\n",
    "\t\t\tNone\n",
    "\t\t\"\"\"\n",
    "\t\t###TODO\n",
    "\t\tself.transition_probas = defaultdict(lambda : defaultdict(float))\n",
    "\t\ttag_freqs = defaultdict(lambda : 0)\n",
    "\t\t\n",
    "\t\ttag_set = set()\n",
    "\t\tfor sent_tags in tags:\n",
    "\t\t\tfor idx,tag in enumerate(sent_tags):\n",
    "\t\t\t\t\n",
    "\t\t\t\ttag_set.add(tag)\n",
    "\t\t\t\t\n",
    "\t\t\t\tif idx < len(sent_tags) - 1:\n",
    "\t\t\t\t\ttag_freqs[tag] += 1\n",
    "\t\t\t\t\n",
    "\t\t\t\tif idx > 0:\n",
    "\t\t\t\t\tself.transition_probas[sent_tags[idx-1]][sent_tags[idx]] += 1\n",
    "\t\t\n",
    "\t\tN = len(tag_set)\n",
    "\t\tnr = 0\n",
    "\t\tdr = 0\n",
    "\t\t\n",
    "        #This is unnecessary...anyways..fuck it\n",
    "\t\tif self.smoothing > 0:\n",
    "\t\t\tnr = self.smoothing\t\t\n",
    "\t\t\tdr = N*self.smoothing\n",
    "\n",
    "\t\tfor tag1 in tag_set:\n",
    "\t\t\tfor tag2 in tag_set:\n",
    "\t\t\t\tself.transition_probas[tag1][tag2] += nr\n",
    "\t\t\t\tself.transition_probas[tag1][tag2] /= (tag_freqs[tag1] + dr)\n",
    "\n",
    "\t\t\t\t\n",
    "\tdef fit_emission_probas(self, sentences, tags):\n",
    "\t\t\"\"\"\n",
    "\t\tEstimate the HMM emission probabilities from the provided data. \n",
    "\n",
    "\t\tCreates a new instance variable called `emission_probas` that is a \n",
    "\t\tdict from a string ('state') to a dict from string to float. E.g.\n",
    "\t\t{'N': {'dog': .1, 'cat': .7, 'mouse': 2},\n",
    "\t\t 'V': {'run': .3, 'go': .5, 'jump': 2},\n",
    "\t\t ...\n",
    "\t\t}\n",
    "\n",
    "\t\tParams:\n",
    "\t\t  sentences...a list of lists of strings, representing the tokens in each sentence.\n",
    "\t\t  tags........a list of lists of strings, representing the tags for one sentence.\n",
    "\t\tReturns:\n",
    "\t\t\tNone\t\t  \n",
    "\n",
    "\t\tSee test_hmm_fit_emission.\n",
    "\t\t\"\"\"\n",
    "\t\t###TODO\n",
    "\t\t\n",
    "\t\tself.emission_probas = defaultdict(lambda : defaultdict(float))\n",
    "\t\ttag_freqs = defaultdict(lambda : 0.0)\n",
    "\t\ttag_set = set()\n",
    "\t\tword_set = set()\n",
    "\t\t\n",
    "\t\tfor sent_tag_tup in zip(sentences,tags):\n",
    "\t\t\tsentence = sent_tag_tup[0]\n",
    "\t\t\ttag_lst  = sent_tag_tup[1]\n",
    "\t\t\t\n",
    "\t\t\tfor word_tag_tup in zip(sentence,tag_lst):\n",
    "\t\t\t\tword = word_tag_tup[0]\n",
    "\t\t\t\ttag = word_tag_tup[1]\t\t\t\t\n",
    "\t\t\t\tword_set.add(word)\n",
    "\t\t\t\ttag_set.add(tag)\n",
    "\t\t\t\ttag_freqs[tag] += 1\n",
    "\t\t\t\t\n",
    "\t\t\t\tself.emission_probas[tag][word] += 1\n",
    "\t\t\n",
    "\t\tN = len(word_set)\n",
    "\t\tnr = 0\n",
    "\t\tdr = 0\n",
    "\t\t\n",
    "\t\tif self.smoothing > 0:\n",
    "\t\t\tnr = self.smoothing\t\t\n",
    "\t\t\tdr = N*self.smoothing\n",
    "\t\t\t\n",
    "\t\tfor word in word_set:\n",
    "\t\t\tfor tag in tag_set:\n",
    "\t\t\t\tself.emission_probas[tag][word] += nr\n",
    "\t\t\t\tself.emission_probas[tag][word] /= (tag_freqs[tag] + dr)\n",
    "\t\t\n",
    "\n",
    "\tdef fit_start_probas(self, tags):\n",
    "\t\t\"\"\"\n",
    "\t\tEstimate the HMM start probabilities form the provided data.\n",
    "\n",
    "\t\tCreates a new instance variable called `start_probas` that is a \n",
    "\t\tdict from string (state) to float indicating the probability of that\n",
    "\t\tstate starting a sentence. E.g.:\n",
    "\t\t{\n",
    "\t\t\t'N': .4,\n",
    "\t\t\t'D': .5,\n",
    "\t\t\t'V': .1\t\t\n",
    "\t\t}\n",
    "\n",
    "\t\tParams:\n",
    "\t\t  tags...a list of lists of strings representing the tags for one sentence.\n",
    "\t\tReturns:\n",
    "\t\t\tNone\n",
    "\n",
    "\t\tSee test_hmm_fit_start\n",
    "\t\t\"\"\"\n",
    "\t\t###TODO\n",
    "\t\t\n",
    "\t\tself.start_probas = defaultdict(lambda : 0.0)\n",
    "\t\tself.tag_set = set()\t\t\n",
    "\t\t\n",
    "\t\tfor tag_lst in tags:\n",
    "\t\t\tfor idx, tag in enumerate(tag_lst):\n",
    "\t\t\t\tself.tag_set.add(tag)\n",
    "\t\t\t\t\n",
    "\t\t\t\tif idx == 0:\n",
    "\t\t\t\t\tself.start_probas[tag] += 1\n",
    "\t\t\n",
    "\t\tdr_lhs = len(tags)\n",
    "\n",
    "\t\tN = len(self.tag_set)\n",
    "\t\tnr = 0\n",
    "\t\tdr = 0\n",
    "\t\t\n",
    "\t\tif self.smoothing > 0:\n",
    "\t\t\tnr = self.smoothing\t\t\n",
    "\t\t\tdr = N*self.smoothing\t\t\n",
    "\t\t\n",
    "\t\tfor tag in self.tag_set:\n",
    "\t\t\tself.start_probas[tag] += nr\n",
    "\t\t\tself.start_probas[tag] /= (dr_lhs + dr)\n",
    "\t\t\t\n",
    "\t\tself.states = sorted(list(self.tag_set))\t\t\n",
    "\t\t\n",
    "\tdef fit(self, sentences, tags):\n",
    "\t\t\"\"\"\n",
    "\t\tFit the parameters of this HMM from the provided data.\n",
    "\n",
    "\t\tParams:\n",
    "\t\t  sentences...a list of lists of strings, representing the tokens in each sentence.\n",
    "\t\t  tags........a list of lists of strings, representing the tags for one sentence.\n",
    "\t\tReturns:\n",
    "\t\t\tNone\t\t  \n",
    "\n",
    "\t\tDONE. This just calls the three fit_ methods above.\n",
    "\t\t\"\"\"\t\t\n",
    "\t\tself.fit_transition_probas(tags)\n",
    "\t\tself.fit_emission_probas(sentences, tags)\n",
    "\t\tself.fit_start_probas(tags)\n",
    "\n",
    "\n",
    "\tdef viterbi(self, sentence):\n",
    "\t\t\"\"\"\n",
    "\t\tPerform Viterbi search to identify the most probable set of hidden states for\n",
    "\t\tthe provided input sentence.\n",
    "\n",
    "\t\tParams:\n",
    "\t\t  sentence...a lists of strings, representing the tokens in a single sentence.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t  path....a list of strings indicating the most probable path of POS tags for\n",
    "\t\t  \t\t  this sentence.\n",
    "\t\t  proba...a float indicating the probability of this path.\n",
    "\t\t\"\"\"\n",
    "\t\t###TODO\t\t\n",
    "\t\t\n",
    "\t\tword_cnt = len(sentence)\n",
    "\t\ttag_cnt  = len(self.states)\n",
    "\t\t\n",
    "\t\t#Specify size\n",
    "\t\tviterbi = np.zeros((tag_cnt, word_cnt))\n",
    "\t\tback_ptr = np.zeros((tag_cnt, word_cnt),dtype=int)\n",
    "\t\t\n",
    "\t\t#Init\n",
    "\t\tq_map = defaultdict(str)\n",
    "\t\tfor id, state in enumerate(self.states):\n",
    "\t\t\tq_map[id] = state\n",
    "\t\t\n",
    "\t\to_map = defaultdict(str)\n",
    "\t\tfor id, word in enumerate(sentence):\n",
    "\t\t\to_map[id] = word\t\t\n",
    "\t\t\n",
    "\t\t#Recurse\n",
    "\t\tfor i in range(viterbi.shape[0]):\n",
    "\t\t\tviterbi[i,0] = self.start_probas[q_map[i]] * self.emission_probas[q_map[i]][o_map[0]]\n",
    "\t\t\tback_ptr[i,0] = 0\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tcols = viterbi.shape[1]\n",
    "\t\trows = viterbi.shape[0]\n",
    "\t\tfor o_id in range(1,cols):\n",
    "\t\t\tfor q_id in range(0,rows):\n",
    "\t\t\t\tmax = -99999\n",
    "\t\t\t\tfor  i in range(0,rows):\n",
    "\t\t\t\t\tcur = viterbi[i,o_id-1]*self.transition_probas[q_map[i]][q_map[q_id]]*self.emission_probas[q_map[q_id]][o_map[o_id]]\t\t\t\t\t\n",
    "\t\t\t\t\tif max < cur:\n",
    "\t\t\t\t\t\tmax = cur\n",
    "\t\t\t\t\t\tb_ptr = i\n",
    "\t\t\t\tviterbi[q_id,o_id] = max\n",
    "\t\t\t\tback_ptr[q_id,o_id] = b_ptr\n",
    "\t\t\n",
    "\t\t#Best_path_prob\n",
    "\t\tmax_prob = np.max((viterbi[:,viterbi.shape[1]-1,]))\n",
    "\t\tmax_prob_idx = np.argmax(viterbi[:,viterbi.shape[1]-1,])\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tpath = []\n",
    "\t\tpath.append(q_map[max_prob_idx])\n",
    "\t\t\n",
    "\t\tcol_cnt = viterbi.shape[1]\n",
    "\t\tfor i in list(range(col_cnt))[0:col_cnt-1]:\t\t\t\n",
    "\t\t\tcol = back_ptr[:,col_cnt-(i+1):col_cnt-i]\n",
    "\t\t\tpath.append(q_map[col[max_prob_idx][0]])\n",
    "\t\t\tmax_prob_idx = col[max_prob_idx][0]\n",
    "\t\t\n",
    "\t\treturn path[::-1], max_prob\t\t\t\t\t\t\t\t \t\t\n",
    "\t\t\n",
    "\n",
    "def read_labeled_data(filename):\n",
    "\t\"\"\"\n",
    "\tRead in the training data, consisting of sentences and their POS tags.\n",
    "\n",
    "\tEach line has the format:\n",
    "\t<token> <tag>\n",
    "\n",
    "\tNew sentences are indicated by a newline. E.g. two sentences may look like this:\n",
    "\t<token1> <tag1>\n",
    "\t<token2> <tag2>\n",
    "\n",
    "\t<token1> <tag1>\n",
    "\t<token2> <tag2>\n",
    "\t...\n",
    "\n",
    "\tSee data.txt for example data.\n",
    "\n",
    "\tParams:\n",
    "\t  filename...a string storing the path to the labeled data file.\n",
    "\tReturns:\n",
    "\t  sentences...a list of lists of strings, representing the tokens in each sentence.\n",
    "\t  tags........a lists of lists of strings, representing the POS tags for each sentence.\n",
    "\t\"\"\"\n",
    "\t###TODO\n",
    "    \n",
    "\tsentences = []\n",
    "\tsent_tags = []\n",
    "\t\n",
    "\twith open(filename,'r') as file:\n",
    "\t\twords = []\n",
    "\t\tword_tags = []\n",
    "\t\tfor line in file:\t\t\t\n",
    "\t\t\tif line != '\\n':\n",
    "\t\t\t\tword_pos = line.strip('\\n').split(' ')\n",
    "\t\t\t\tword = word_pos[0]\n",
    "\t\t\t\tpos  = word_pos[1]\t\n",
    "\t\t\t\t\n",
    "\t\t\t\twords.append(word)\n",
    "\t\t\t\tword_tags.append(pos)\n",
    "\t\t\telse:\n",
    "\t\t\t\tsentences.append(words)\n",
    "\t\t\t\tsent_tags.append(word_tags)\n",
    "\t\t\t\t\n",
    "\t\t\t\twords = []\n",
    "\t\t\t\tword_tags = []\n",
    "\t\n",
    "\treturn sentences, sent_tags\n",
    " \n",
    "\t\t\t\t\t\t\n",
    "\n",
    "def download_data():\n",
    "    \"\"\" Download labeled data.\n",
    "    DONE ALREADY.\n",
    "    \"\"\"\n",
    "    url = 'https://www.dropbox.com/s/ty7cclxiob3ajog/data.txt?dl=1'\n",
    "    urllib.request.urlretrieve(url, 'data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 34 states\n",
      "['$', \"''\", ',', '.', ':', 'CC', 'CD', 'DT', 'EX', 'IN', 'JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'TO', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WRB', '``']\n"
     ]
    }
   ],
   "source": [
    "fname = 'data.txt'\n",
    "if not os.path.isfile(fname):\n",
    "    download_data()\n",
    "sentences, tags = read_labeled_data(fname)\n",
    "\n",
    "model = HMM(.001)\n",
    "model.fit(sentences, tags)\n",
    "print('model has %d states' % len(model.states))\n",
    "print(model.states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted parts of speech for the sentence ['Look', 'at', 'what', 'happened']\n",
      "(['VB', 'IN', 'WP', 'VBD'], 2.751820088075314e-10)\n"
     ]
    }
   ],
   "source": [
    "sentence = ['Look', 'at', 'what', 'happened']\n",
    "print('predicted parts of speech for the sentence %s' % str(sentence))\n",
    "print(model.viterbi(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
